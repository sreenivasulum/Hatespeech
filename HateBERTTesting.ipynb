{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.14.0-cp311-cp311-macosx_10_15_x86_64.whl (229.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.7/229.7 MB\u001b[0m \u001b[31m294.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:22\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.0.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow)\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m292.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Downloading libclang-16.0.6-py2.py3-none-macosx_10_9_x86_64.whl (24.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m438.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting ml-dtypes==0.2.0 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.2.0-cp311-cp311-macosx_10_9_universal2.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m416.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.23.5 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m459.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.24.3)\n",
      "Requirement already satisfied: setuptools in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (4.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.34.0-cp311-cp311-macosx_10_14_x86_64.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m312.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (1.59.0)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorflow) (2.14.1)\n",
      "Collecting tensorflow-estimator<2.15,>=2.14.0 (from tensorflow)\n",
      "  Downloading tensorflow_estimator-2.14.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.7/440.7 kB\u001b[0m \u001b[31m624.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting keras<2.15,>=2.14.0 (from tensorflow)\n",
      "  Downloading keras-2.14.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m440.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.23.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/centific/anaconda3/lib/python3.11/site-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/centific/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/centific/anaconda3/lib/python3.11/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/centific/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/centific/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/centific/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/centific/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/centific/anaconda3/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/centific/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, tensorflow-io-gcs-filesystem, tensorflow-estimator, opt-einsum, ml-dtypes, keras, google-pasta, gast, astunparse, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-23.5.26 gast-0.5.4 google-pasta-0.2.0 keras-2.14.0 libclang-16.0.6 ml-dtypes-0.2.0 opt-einsum-3.3.0 tensorflow-2.14.0 tensorflow-estimator-2.14.0 tensorflow-io-gcs-filesystem-0.34.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras-Preprocessing\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m593.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /Users/centific/anaconda3/lib/python3.11/site-packages (from Keras-Preprocessing) (1.24.3)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/centific/anaconda3/lib/python3.11/site-packages (from Keras-Preprocessing) (1.16.0)\n",
      "Installing collected packages: Keras-Preprocessing\n",
      "Successfully installed Keras-Preprocessing-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras-Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "gather": {
     "logged": 1673803576399
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "#import emoji\n",
    "import torch\n",
    "from tensorflow import keras\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import pandas as pd \n",
    "import datetime\n",
    "from _datetime import datetime as dt\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelWithLMHead\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    WEIGHTS_NAME,\n",
    "    AdamW,\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizer,\n",
    "    #CamembertConfig,\n",
    "    #CamembertForMaskedLM,\n",
    "    #CamembertTokenizer,\n",
    "    DistilBertConfig,\n",
    "    DistilBertForMaskedLM,\n",
    "    DistilBertTokenizer,\n",
    "    GPT2Config,\n",
    "    GPT2LMHeadModel,\n",
    "    GPT2Tokenizer,\n",
    "    OpenAIGPTConfig,\n",
    "    OpenAIGPTLMHeadModel,\n",
    "    OpenAIGPTTokenizer,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    RobertaConfig,\n",
    "    RobertaForMaskedLM,\n",
    "    RobertaTokenizer,\n",
    "    #get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "gather": {
     "logged": 1673803576872
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def load_train_test_data1(tokenizer,Text):\n",
    "\n",
    "    Data=[]\n",
    "    count=0\n",
    "    Label=[]\n",
    "    for i in Text:\n",
    "        #print (i)\n",
    "        full_line = str(i)\n",
    "        #full_line = re.sub(r'#([^ ]*)', r'\\1', full_line)\n",
    "        full_line = re.sub(r'https.*[^ ]', 'URL', full_line)\n",
    "        full_line = re.sub(r'http.*[^ ]', 'URL', full_line)\n",
    "        #full_line = emoji.demojize(full_line)\n",
    "        full_line = re.sub(r'(:.*?:full_line)', r' \\1 ', full_line)\n",
    "        full_line = re.sub(' +', ' ', full_line)\n",
    "        full_line=re.sub(\"\\s\\s+\" , \" \", full_line)\n",
    "        full_line = re.sub( '[^a-zA-Z]', ' ', full_line)\n",
    "        if len(list(full_line.split()))>1:\n",
    "            #print(f'{count}.{full_line}')\n",
    "            Data.append(full_line)\n",
    "#Label=list(df5['Labels'])\n",
    "\n",
    "    \n",
    "    test_tweets=Data\n",
    "    # List of all tokenized tweets\n",
    "    test_input_ids = []\n",
    "\n",
    "    # For every tweet in the test set\n",
    "    for sent in test_tweets:\n",
    "        # `encode` will:\n",
    "        #   (1) Tokenize the sentence.\n",
    "        #   (2) Prepend the `[CLS]` token to the start.\n",
    "        #   (3) Append the `[SEP]` token to the end.\n",
    "        #   (4) Map tokens to their IDs.\n",
    "        encoded_sent = tokenizer.encode(\n",
    "            sent,  # Sentence to encode.\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=100, # orignal value 512 - change Tommaso\n",
    "\n",
    "            # This function also supports truncation and conversion\n",
    "            # to pytorch tensors, but we need to do padding, so we\n",
    "            # can't use these features :( .\n",
    "            # max_length = 128,          # Truncate all sentences.\n",
    "            # return_tensors = 'pt',     # Return pytorch tensors.\n",
    "        )\n",
    "\n",
    "        # Add the encoded tweet to the list.\n",
    "        test_input_ids.append(encoded_sent)\n",
    "\n",
    "    # # Pad our input tokens with value 0.\n",
    "    # # \"post\" indicates that we want to pad and truncate at the end of the sequence,\n",
    "    # # as opposed to the beginning.\n",
    "\n",
    "    test_input_ids = pad_sequences(test_input_ids, maxlen=100, dtype=\"long\",\n",
    "                                    value=tokenizer.pad_token_id, truncating=\"pre\", padding=\"pre\")\n",
    "\n",
    "\n",
    "\n",
    "    # Create attention masks\n",
    "    # The attention mask simply makes it explicit which tokens are actual words versus which are padding\n",
    "    test_attention_masks = []\n",
    "\n",
    "    # For each tweet in the test set\n",
    "    for sent in test_input_ids:\n",
    "        # Create the attention mask.\n",
    "        #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
    "        #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
    "        att_mask = [int(token_id > 0) for token_id in sent]\n",
    "\n",
    "        # Store the attention mask for this sentence.\n",
    "        test_attention_masks.append(att_mask)\n",
    "\n",
    "    # Return the list of encoded tweets, the list of labels and the list of attention masks\n",
    "    return  test_input_ids, test_attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "gather": {
     "logged": 1673797710641
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['                   you ve been a good as friend to me   glad I got your honkie ass  Let s fuck shit up this year',\n",
       " 'Scandal  A Negro Bed Wench s Fairy tale and propaganda  URL',\n",
       " 'when bitches change and start acting like the nigga they with  lt  lt  lt  lt  lt  lt ',\n",
       " 'She even had a hoe name ',\n",
       " 'Golden Oreos are the best',\n",
       " ' VNNForum These Marxist kikes promoted Anti Semitism by creating this film  what was their incentive  They just promote their enemies ',\n",
       " ' USER  USER  USER  USER He is an awesome dog  Almost worth the trip',\n",
       " ' USER Yes everyone does but he is in the best position and missing all over the place',\n",
       " ' GrizzboAdams ight nigga haha fucking queer has to clear his schedule',\n",
       " ' USER  USER I agree he is very intelligent and I like the latest policy of a sovereign wealth fund   something I ve long supported  But he just doesn t cut through  Someone like Jo Swinson would get the party more air time imo ']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1673803593525
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. when bitches change and start acting like the nigga they with  lt  lt  lt  lt  lt  lt = Hate\n",
      "2. Scandal  A Negro Bed Wench s Fairy tale and propaganda  URL= Hate\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#model_dir='../Data/2023-01-04_Covidtweets'\n",
    "model_dir='2022-12-29_Hatedata'\n",
    "\n",
    "Text=[\"when bitches change and start acting like the nigga they with  lt  lt  lt  lt  lt  lt \", 'Scandal  A Negro Bed Wench s Fairy tale and propaganda  URL']\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained Tokenizer from directory, change this to load a tokenizer from ber package\n",
    "tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "# Load a trained model and vocabulary that you have fine-tuned\n",
    "model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "#tokenizer = tokenizer_class.from_pretrained(model_dir)\n",
    "\n",
    "# Copy the model to the GPU.\n",
    "#model.to(device)\n",
    "\n",
    "# Set the model to work on CPU if no GPU is present\n",
    "model.to(device)\n",
    "\n",
    "validation_inputs, validation_masks = load_train_test_data1(tokenizer,Text)\n",
    "\n",
    "\n",
    "# Tweets\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "\n",
    "# Attention masks\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Create the DataLoader for our validation set.\n",
    "#validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "# Note that the number of batch has to be the same, this means that we have to aggregate results in the end\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    # Store  predicted labels for global eval\n",
    "predicted_labels = []\n",
    "\n",
    "t0 = time.time()\n",
    "model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "for batch in validation_dataloader:\n",
    "        # Add batch to GPU/CPU\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    #batch=tuple(t.to(device))\n",
    "\n",
    "        # Unpack the inputs from our dataloader\n",
    "    #b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask = batch\n",
    "\n",
    "        # Telling the model not to compute or store gradients, saving memory and\n",
    "        # speeding up validation\n",
    "    with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions.\n",
    "            # This will return the logits rather than the loss because we have\n",
    "            # not provided labels.\n",
    "            # token_type_ids is the same as the \"segment ids\", which\n",
    "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
    "            # The documentation for this `model` function is here:\n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "        outputs = model(b_input_ids,\n",
    "                            token_type_ids=None,\n",
    "                            attention_mask=b_input_mask)\n",
    "\n",
    "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "        # values prior to applying an activation function like the softmax.\n",
    "    logits = outputs[0]\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    #label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Calculate the accuracy for this batch of test sentences.\n",
    "    #tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "        # Accumulate the total accuracy.\n",
    "    #eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        # Track the number of batches\n",
    "    #nb_eval_steps += 1\n",
    "\n",
    "    pred_flat = np.argmax(logits, axis=1).flatten()\n",
    "    #labels_flat = label_ids.flatten()\n",
    "\n",
    "       # Store gold labels single list\n",
    "    #gold_labels.extend(labels_flat)\n",
    "        # Store predicted labels single list\n",
    "    predicted_labels.extend(pred_flat)\n",
    "count=0\n",
    "for i,j in zip(predicted_labels,Text):\n",
    "    count=count+1\n",
    "    if (i==0):\n",
    "        print (f\"{count}. {j}= Non-Hate\")\n",
    "    else:\n",
    "        print (f\"{count}. {j}= Hate\")\n",
    "#print (predicted_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "hate"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   },
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
